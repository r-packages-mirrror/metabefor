---
title: "Tutorial: Doing a Scoping Review with metabefor"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tutorial: Doing a Scoping Review with metabefor}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This tutorial assumes that you are familiar with the Inclusive Systematic Review Registration Form. This form is available in [this Google Doc](https://docs.google.com/document/d/1BFrmEWGz9Zb_vssC5simJ6ohuid3z-LitL99WRxFnE4/edit?usp=sharing), in [this MetArXiv preprint](https://doi.org/10.31222/osf.io/3nbea), and in [the `preregr` package](https://r-packages.gitlab.io/preregr/articles/form_inclSysRev_v0_92.html). I also strongly recommend using that form to document your plans before starting any "actual work" on the review. The tutorial describes parts of the process of planning the review, as well as conducting the review.

This tutorial will use terms defined in the [Definitions vignette](https://r-packages.gitlab.io/metabefor/articles/definitions.html). That list of definitions is probably very hard to follow if you read it without context, but hopefully this tutorial will provide the necessary context.

## Scoping Reviews

Scoping reviews or evidence maps (depending on who you ask, these can be the same or slightly different) differ from most types of systematic reviews in that they don't answer substantive research questions (note that when I use "systematic reviews", that also includes meta-analyses). Instead, they provide an overview of the scope of the literature: in a sense, they can tell you which research questions _can_ be answered with systematic reviews.

Where most systematic reviews synthesize the evidence itself, often aiming to provide a more conclusive answer to the same or similar research questions as the included primary studies asked, scoping reviews synthesize the metadata about that evidence. They can tell you things like when most studies were conducted; which (sub)topics received most attention when; which study designed were used and whether that was associated to (sub)topic; how studies were distributed geographically; which sample sizes were common; whether any of those variables shows trends over time; et cetera.

Scoping reviews also produce an extensive database of literature, and are an excellent starting point for focused systematic reviews. Those also become much easier to plan, since you'll know how many studies are available. Depending on the comprehensiveness of your scoping review's extraction, you may even be able to skip the search and screening phases of those systematic reviews, since you already know which articles to include. Especially in combination with a decentralized approach to extraction, this means that scoping reviews can enable very efficient mapping of the literature.

## Research questions

Research questions in scoping reviews ask what researchers did. They can concern anything from whether different geographical regions prioritize different topics, whether sample sizes increased or decreased over time, which definitions researchers use, and which measurement instruments researchers use, via things like which study designs are used to answer which types of questions and how paradigms change over time to whether shifts occurred in researchers' underlying philosophy of science or epistemological perspectives.

These research questions will always contain one or more concepts. Each of this concepts will relate to one or more [entities to extract](https://r-packages.gitlab.io/metabefor/articles/definitions.html) (see below). Once you have decided on your research question, therefore, you can decide on the entities to extract.

However, in practice this process is nonlinear and iterative. Any given research question (or more accurately, any given set of entities to extract) implicitly determines the scope of the review, because the exclusion criteria are based on the research questions and the entities to extract, and the search strategy (e.g. the query) is based on the research questions, the entities to extract, and the exclusion criteria. As such, there is always some correspondence between the research question and the number of sources that your search strategy will yield (and that will have to be screened).

Therefore, you will usually develop all of these in parallel. For example, it is common to test different queries until the number of hits is feasible given the available resources. Depending on your screening capacity, you may be forced to revise and limit the scope of the research question(s). Similarly, if your query yields very few hits, you may want or need to broaden it (and therefore, broaden your research question(s)) to eventually obtain worthwhile results.

## Extraction

The extraction is the stage where the "data" are extracted from the identified [sources](https://r-packages.gitlab.io/metabefor/articles/definitions.html). This means that the information from or about the included sources has to be stored in an extraction script file.

In a systematic review, the extraction and synthesis stages are the hardest (unless a meta-analysis is possible, in which case the extraction stage is the hardest). This is because the information to extract is often ambigous and sometimes not available at all. This ubiquitous ambiguity means that the task of extracting information is typically not a matter of copying information over: instead it's more like playing detective.

Planning the extraction is also the hardest part of planning a systematic review. Planning the extraction means specifying the R extraction script, which requires specifying the entities to extract, how they are hierarchically organized, and which value templates each entity uses.

An entity is anything that has to be extracted from a source, such as the year something was published, its authors, a definition that was used in a source, a theory that was studied, a study design, a sample size, a measurement instrument, the literal text of measurement instrument items, expressions from interview participants, effect sizes, or things like the number or figures, tables or words in a source.

### Extraction & categorization

For some entities, the potential values an extracted entity can take are knowable in advance in a given systematic review context. For example, when reviewing primary studies in humans or other animals, the sample size must be a positive integer (e.g. 1, 2, 3, ...); and publication year will usually have to be a positive integer, often of four digits. In other cases, it is clear that free text will be extracted, for example, when author names or source titles are extracted.

For many entities, however, it is less obvious how to operationalize them. When something is extracted as free text, often as many unique values will be extracted as there are sources. This means that synthesis (i.e., "analysis", see below) first requires transformation of those values. A list of raw free text values cannot be synthesized: the strings of characters have no encoded meaning, and cannot be collapsed or summarized. Nothing can be calculated from a list of free text values; and if that list is used in a table, that table will have as many rows or columns as the list of free text values has different values. Especially in scoping reviews, where including hundreds of sources can be quite common, this often isn't feasible.

In many cases, this problem can be avoided by having extracters categorize that information during extraction. For example, imagine a scoping review into qualitative research practices in a given field. One of the entities that will be extracted is how the researchers coded the data. In this case, an infinite number of coding approaches can be used. Many textbooks on qualitative research use some categorizations to organize these. For example, coding can be categorized as "inductive" versus "deductive" or as "open" versus "axial". Like any categorization, these simplify reality, making it easier to deal with for humans. If this simplification is not problematic given the scoping reviewers' research question(s), they can choose to adopt one of those categorizations.

In that case, they would decide which categories to use (e.g. `inductive coding` and `deductive coding`, or two symbols representing these two categories, such as `1` and `2`) and specify clear coding instructions for each (often with special attention to edge cases). After extraction, instead of having one or several sentences of free text extracted for each source (where the original authors describe their coding approach), they would then have a list with only possible two values (e.g. `inductive coding` and `deductive coding`, or `1` and `2`). This lends itself to easy synthesis: the percentage of sources using inductive coding could easily be obtained, and it would be possible to answer questions such as whether that percentage seems stable over time, or differs between subdomains, or by geographical area.

### Ambiguity

However, the extractors would also encounter sources where the authors used both types of coding - and they would encounter sources where a coding approach would be used that could arguably belong in either (or neither) category. There are two strategies to try and prevent such problems.

The first is developing very, very comprehensive coding instructions. If the scoping reviewers have a clear idea of all potential coding approaches, discussing all edge cases extensively in the coding instructions can ensure unequivocal (and correct) categorizations of most potential descriptions extractors can encounter in the sources. For example, the coding instructions can instruct extractors to categorize all sources using both inductive and deductive coding as "inductive" (or "deductive", depending on what makes sense given the scoping review's goals).

The second is putting a lot of thought into the categories that are used for each entity. For example, instead of using two categories, the scoping reviewers could add a third category `inductive and deductive coding`. They could also split the entity into two dichotomous entities, having extractors extract whether inductive coding was used into one, and whether deductive coding was used into another. By adding a third category `unclear` to each entity, ambiguous cases could be easily spotted - however, at the cost of no longer knowing what the extractor would guess if forced. That could be solved by adding more categories, for example extracting the entity `inductive coding` into categories `no`, `unlikely`, `likely`, and `yes`; or, alternatively, by adding a second entity that holds the extractor's confidence in the categorization.

### The cost of categorization

Each of these solutions to the problems caused by reality (including researchers' decisions as extracted in scoping reviews) usually not being neatly organized into categories entail some costs. The more entities that are used to store the information extracted from the sources, and the more categories that are used for each entity, the less information is lost during extraction -- but the more time and effort the extraction costs.

In addition, any categorization by definition means that what can be learned from the scoping review is limited to the "potential answer space" formed by what the scoping reviewers knew a priori. If a research question is "which coding approaches are used", and the entities that scoping reviewers extract into are `inductive coding` and `deductive coding` (both with categories `no`, `unlikely`, `likely`, and `yes`), then the synthesis can never result in conclusions about the proportion of sources where the researchers reported they used guinnea pigs, neural networks, or magic crystals for coding, even if a sizeable proportion of the sources reports those approaches. Each of these three types of coding approaches will instead be categorized as either inductive coding or deductive coding (or potentially both) -- if the coding instructions are of sufficient quality, they will be categorized unequivocally and consistently, but still, a lot of information will be lost.

This can be problematic depending on the research questions. Often, what the scoping reviewers *do not* see coming a priori can be the most interesting. When the nature or scope of the "potential answer space" is not the thing of interest (i.e., the researchers are interested in where the set of included sources falls in that space), the costs of categorization can be zero or low. However, when it is not clear in advance how that space looks, researchers may not be able to afford categorization at extraction time.

### Coding after extraction

In that case, 





For example, they can be categorized into a smaller number of categories, or natural language processing algorithms can be applied to infer some specific attribute. The latter is only possible if the information of interest is very rudimentary (e.g. estimations of sentiment or complexity). It is more common that you're interested in something computers cannot understand, in which case those free text values have to be coded by a human (or several).



Extracting free text and then having a separate coding stage has one drawback: the higher cost (i.e. in terms of time, energy, money). It has two advantages: flexibility and transparency. 



open vs closed



https://r-packages.gitlab.io/metabefor/articles/definitions.html

### Entities


### Instructions



